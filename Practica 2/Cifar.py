# -*- coding: utf-8 -*-
"""Cifar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Ludvins/VC/blob/master/Practica%202/Keras.ipynb

## Imports
"""

import numpy as np
import keras
import matplotlib.pyplot as plt
import keras.utils as np_utils
# Import optimizer
from keras.optimizers import SGD
# Import data
from keras.datasets import cifar100
# Import models and layers
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, Conv1D, GlobalAveragePooling2D
# Import image proprocessors
from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
# Import Resnet
from keras.applications.resnet50 import ResNet50, preprocess_input
# Import Early Stopping
from keras.callbacks import EarlyStopping
# Show no TensorFlow deprecation warnings

"""
Loads Cifar data.
"""

def load_cifar_data():
    # Load Data
    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')

    # Transform to float and normalize
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    
    # We will use only 25 classes.
    train_idx = np.isin(y_train, np.arange(25))
    train_idx = np.reshape(train_idx, -1)
    x_train = x_train[train_idx]
    y_train = y_train[train_idx]
    test_idx = np.isin(y_test, np.arange(25))
    test_idx = np.reshape(test_idx, -1)
    x_test = x_test[test_idx]
    y_test = y_test[test_idx]
    
    # Set labels to cathegorical
    y_train = np_utils.to_categorical(y_train, 25)
    y_test = np_utils.to_categorical(y_test, 25)
    
    return x_train, y_train, x_test, y_test

"""# Functions for graphics

Draws two graphics.
+ Loss function evolution.
+ Accuracy function evolution.

Args:
+ ```hist```: Training records
"""

def show_evolution(hist):
    loss = hist.history['loss']
    val_loss = hist.history['val_loss']
    plt.plot(loss)
    plt.plot(val_loss)
    plt.legend(['Training loss', 'Validation loss'])
    plt.show()
    
    acc = hist.history['accuracy']
    val_acc = hist.history['val_accuracy']
    plt.plot(acc)
    plt.plot(val_acc)
    plt.legend(['Training accuracy', 'Validation accuracy'])
    plt.show()

"""Draws the same graphics as ```show_evolution``` but using a list of records ```hist```."""

def compare_evolution(hist, names):

    for i in hist:
        val_loss = i.history['val_loss']
        plt.plot(val_loss)

    plt.legend(["Validation loss " + names[i] for i in range(len(hist))])
    plt.show()

    for i in hist:
        val_acc = i.history['val_accuracy']
        plt.plot(val_acc)

    plt.legend(["Validation accuracy " + names[i] for i in range(len(hist))])
    plt.show()

"""# Data generators"""

def sample_data_generator():
  return ImageDataGenerator(
      featurewise_center = True,             # set input mean to 0 over the dataset
      samplewise_center = False,             # set each sample mean to 0
      featurewise_std_normalization = True,  # divide inputs by std of the dataset
      samplewise_std_normalization = False,  # divide each input by its std
      zca_whitening = False,                 # apply ZCA whitening
      zca_epsilon = 1e-06,                   # epsilon for ZCA whitening
      rotation_range = 0,                    # randomly rotate images in the range (degrees, 0 to 180)
      width_shift_range = 0.1,               # randomly shift images horizontally (fraction of total width)
      height_shift_range = 0.1,              # randomly shift images vertically (fraction of total height)
      shear_range = 0.,                      # set range for random shear
      zoom_range = 0.,                       # set range for random zoom
      channel_shift_range = 0.,              # set range for random channel shifts
      fill_mode = 'nearest',                 # set mode for filling points outside the input boundaries
      cval = 0.,                             # value used for fill_mode = "constant"
      horizontal_flip = True,                # randomly flip images
      vertical_flip = False,                 # randomly flip images
      rescale = None,                        # set rescaling factor (applied before any other transformation)
      preprocessing_function = None,         # set function that will be applied on each input
      data_format = None,                    # image data format, either "channels_first" or "channels_last"
      validation_split = 0.1)                # fraction of images reserved for validation (strictly between 0 and 1)

def train_data_generator_with_whitening():
  return ImageDataGenerator(
      featurewise_center = True,             
      zca_whitening = True,                 
      horizontal_flip = True,             
      validation_split = 0.1)

def test_data_generator_with_whitening():
  return ImageDataGenerator(
      featurewise_center = True,             
      zca_whitening = True)

def train_data_generator_without_whitening():
  return ImageDataGenerator(
      featurewise_center = True,             
      featurewise_std_normalization = True, 
      width_shift_range = 0.1,               
      height_shift_range = 0.1,              
      horizontal_flip = True,               
      validation_split = 0.1)

def test_data_generator_without_whitening():
  return ImageDataGenerator(
      featurewise_center = True,             
      featurewise_std_normalization = True)

"""# Model definitions

## BaseNet Model
"""

def simple_base_net_model():
    model = Sequential()
    model.name = "Modelo BaseNet"
    model.add(Conv2D(6, kernel_size=(5, 5),
                     activation='relu',
                     input_shape=(32,32,3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(16, kernel_size=(5,5), 
                    activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(50, activation='relu'))
    model.add(Dense(25, activation='softmax'))
    
    return model

def complex_base_net_model_no_norm():
    model = Sequential()
    model.name = "Modelo BaseNet sin BatchNormalization"
    model.add(Conv2D(32, (3, 3), padding='same', activation = 'relu', input_shape=(32,32,3)))
    model.add(Conv2D(32, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(25, activation='softmax'))

    return model

def complex_base_net_model_norm_after_relu():
    model = Sequential()
    model.name = "Modelo BaseNet con BatchNormalization despues de Activacion"
    model.add(Conv2D(32, (3, 3), padding='same', activation = 'relu', input_shape=(32,32,3)))
    model.add(BatchNormalization())
    model.add(Conv2D(32, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(25, activation='softmax'))

    return model

def complex_base_net_model_norm_before_relu():
    model = Sequential()
    model.name = "Modelo BaseNet con BatchNormalization antes de Activacion"
    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(32, (3, 3)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(64, (3, 3)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(25, activation='softmax'))

    return model


"""# Training and evaluation function

Compiles the given model
"""

def model_compile(model):
    model.compile(
        loss = keras.losses.categorical_crossentropy,
        optimizer = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True),
        metrics = ['accuracy']
    )

"""Fits the datagen given by `data_generator` to the given set."""

def fitted_datagen(data_generator, x_train):
    datagen = data_generator()
    datagen.fit(x_train)

    return datagen

"""Fits the model using `train_datagen` if given."""

def train(model, x_train, y_train, batch_size, epochs, train_datagen = None, verbose = 1, val_split = 0.1, shuffle = True):

    if (verbose != 0):
      print(" - ENTRENAMIENTO - ")

    if train_datagen is None:
      return model.fit(x_train, y_train, batch_size, epochs, verbose = verbose, validation_split = val_split, shuffle = shuffle)

    else:
      return model.fit_generator(
                generator = train_datagen.flow(x_train, y_train, batch_size, subset='training'),
                steps_per_epoch = len(x_train)*0.9/batch_size,
                epochs = epochs,
                validation_data = train_datagen.flow(x_train, y_train, batch_size, subset='validation'),
                validation_steps = len(x_train)*0.1/batch_size,
                verbose = verbose,
                callbacks = [EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)],
                shuffle = shuffle
                )

"""Evaluates the model using `train_datagen` if given."""

def evaluate(model, x_test, y_test, test_datagen = None, verbose = 0):
    if (verbose != 0):
      print(" - RESULTADOS - ")
    if test_datagen is None:
      score = model.evaluate(x_test, y_test, verbose = verbose)
    else:
      score = model.evaluate_generator(
        generator = test_datagen.flow(x_test, y_test, 1),
        verbose = verbose
      )
    
    print('PÉRDIDA: ', score[0])
    print('PRECISIÓN: ', score[1])


"""# Test functions

## BaseNet & Cifar
"""

cifar_train, cifar_train_labels, cifar_test, cifar_test_labels = load_cifar_data()

def apartado1():
    print("--- APARTADO 1. ---")
    m = simple_base_net_model()
    model_compile(m)
    basic_model_weights = m.get_weights()
    m.summary()

    h = train(m, cifar_train, cifar_train_labels, 32, 20, verbose = 1)
    evaluate(m, cifar_test, cifar_test_labels, verbose = 0)

    print(" - GRÁFICAS - ")
    show_evolution(h)

    print("--- COMPARACIÓN DE GENERADORES ---")
    print(" - RESULTADOS SIN WHITENING - ")
    m.set_weights(basic_model_weights)

    train_datagen = fitted_datagen(train_data_generator_without_whitening, cifar_train)
    test_datagen = fitted_datagen(test_data_generator_without_whitening, cifar_train)

    h2 = train(m, cifar_train, cifar_train_labels, 32, 20, train_datagen, 0)
    evaluate(m, cifar_test, cifar_test_labels, test_datagen, 0)

    print(" - RESULTADOS CON WHITENING - ")
    m.set_weights(basic_model_weights)

    train_datagen = fitted_datagen(train_data_generator_with_whitening, cifar_train)
    test_datagen = fitted_datagen(test_data_generator_with_whitening, cifar_train)

    h3 = train(m, cifar_train, cifar_train_labels, 32, 20, train_datagen, 0)
    evaluate(m, cifar_test, cifar_test_labels, test_datagen, 0)

    print(" - GRÁFICAS COMPARATIVAS -")
    compare_evolution([h, h2, h3], ["básico", "complejo no whitening", "complejo whitening"])

def apartado2():
    print(" --- BASENET MODIFICADO SIN BATCHNORMALIZATION ---")
    m2 = complex_base_net_model_no_norm()
    model_compile(m2)
    m2.summary()
    complex_model_weights = m2.get_weights()

    train_datagen = fitted_datagen(train_data_generator_without_whitening, cifar_train)
    test_datagen = fitted_datagen(test_data_generator_without_whitening, cifar_train)

    h = train(m2, cifar_train, cifar_train_labels, 32, 100, train_datagen, 0)
    evaluate(m2, cifar_test, cifar_test_labels, test_datagen, 0)
    print(" - GRÁFICAS DE RESULTADOS - ")
    show_evolution(h)

    print(" --- BASENET MODIFICADO CON BATCHNORMALIZATION ANTES VS DESPUES --- ")
    print(" -- BATCHNORMALIZATION ANTES -- ")
    m3 = complex_base_net_model_norm_before_relu()
    model_compile(m3)
    h3 = train(m3, cifar_train, cifar_train_labels, 32, 100, train_datagen, 0)
    evaluate(m3, cifar_test, cifar_test_labels, test_datagen, 0)
    print(" - GRÁFICAS RESULTADOS - ")
    show_evolution(h3)

    print(" -- BATCHNORMALIZATION DESPUÉS -- ")
    m4 = complex_base_net_model_norm_after_relu()
    model_compile(m4)
    h4 = train(m4, cifar_train, cifar_train_labels, 32, 100, train_datagen, 0)
    evaluate(m4, cifar_test, cifar_test_labels, test_datagen, 0)
    print(" - GRÁFICAS RESULTADOS - ")
    show_evolution(h4)

    print(" - GRÁFICAS CONJUNTAS - ")
    compare_evolution([h3, h4], ["Antes", "Después"])

def apartado2a():
    print(" -- BATCHNORMALIZATION DESPUÉS Y WHITENING -- ")
    m5 = complex_base_net_model_norm_after_relu()
    model_compile(m5)
    train_datagen = fitted_datagen(train_data_generator_with_whitening, cifar_train)
    test_datagen = fitted_datagen(test_data_generator_with_whitening, cifar_train)

    h = train(m5, cifar_train, cifar_train_labels, 32, 100, train_datagen, 0)
    evaluate(m5, cifar_test, cifar_test_labels, test_datagen, 0)
    print(" - GRÁFICAS RESULTADOS - ")
    show_evolution(h)


apartado1()
apartado2()
apartado2a()
